citation("rvest")
citation("simputation")
getwd()
citation("boruta")
citation("Boruta")
citation("caret")
citation("GGally")
citation("feasts")
citation("rsample")
citation("caTools")
citation("rpart")
df_households <- read.csv(”data/realestate.csv”)
df_households <- read.csv("data/realestate.csv")
head(df_households)
library(glmnet)
library(dplyr)
# Reading and splitting the data.
df_households <- read.csv("data/realestate.csv")
colnames(df_households_filtered)[6] <- "price"
data_split <- initial_split(df_households, prop = .7)
train <- training(data_split)
test <- testing(data_split)
library(rsample)
set.seed(1234)
# Reading and splitting the data.
df_households <- read.csv("data/realestate.csv")
colnames(df_households_filtered)[6] <- "price"
data_split <- initial_split(df_households, prop = .7)
train <- training(data_split)
test <- testing(data_split)
# Reading and splitting the data.
df_households <- read.csv("data/realestate.csv")
colnames(ddf_households)[6] <- "price"
data_split <- initial_split(df_households, prop = .7)
train <- training(data_split)
test <- testing(data_split)
# Reading and splitting the data.
df_households <- read.csv("data/realestate.csv")
colnames(df_households)[6] <- "price"
data_split <- initial_split(df_households, prop = .7)
train <- training(data_split)
test <- testing(data_split)
# Defining the feature variables.
x <- model.matrix(price~., train)[,c(-1,-2)]
# Defining the class variable.
y <- train$price
x
train
head(train)
x <- model.matrix(price~., train)[,c(-1,-2)]
head(x)
x <- model.matrix(price~., train)[,c(-1,-2)]
x
head(x)
# Reading and splitting the data.
df_households <- read.csv("data/realestate.csv")
df_households_filtered <- df_households[, c(-1,-2)]
colnames(df_households_filtered)[6] <- "price"
data_split <- initial_split(df_households_filtered, prop = .7)
train <- training(data_split)
test <- testing(data_split)
# Defining the feature variables.
x <- model.matrix(price~., train)
# Defining the class variable.
y <- train$price
# Use cross validation to determine the optimal lambda.
cv <- cv.glmnet(x, y, alpha = 1)
best_model <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
x.test <- model.matrix(price~ ., test)[,-1][,-2]
head(x.test)
head(test)
head(df_households_filtered)
head(test)
best_model <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
x.test <- model.matrix(price~ ., test)
price_predictions <- best_model %>% predict(x.test) %>% as.numeric()
price_predictions
test$price
# Model performance summary.
data.frame(
RMSE = RMSE(price_predictions, test$price),
Rsquare = R2(price_predictions, test$price)
)
library(glmnet)
data.frame(
RMSE = RMSE(price_predictions, test$price),
Rsquare = R2(price_predictions, test$price)
)
length(price_predictions)
length(test$price)
library(mlbench)
library(caret)
# Model performance summary.
data.frame(
RMSE = RMSE(price_predictions, test$price),
Rsquare = R2(price_predictions, test$price)
)
library(caret)
library(rsample)
# Reading the data.
df_households <- read.csv("data/realestate.csv")
df_households_filtered <- df_households[, c(-1,-2)]
# Splitting the data.
colnames(df_households_filtered)[6] <- "price"
data_split <- initial_split(df_households_filtered, prop = .7)
train <- training(data_split)
test <- testing(data_split)
mlp_fit <- train(price ~ ., train, method = "nnet",trControl = trainControl(method = "cv",
number = 10))
head(df_households_filtered)
print(mlp_fit)
mlp_fit <- nnet(price ~ ., train, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
library(nnet)
mlp_fit <- nnet(price ~ ., train, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
print(mlp_fit)
citation("glmnet")
citation("factoextra")
library(keras)
library(rsample)
library(dplyr)
# Reading the data.
df_households <- read.csv("data/realestate.csv")
df_households_filtered <- df_households[, c(-1,-2)]
# Splitting the data.
colnames(df_households_filtered)[6] <- "price"
data_split <- initial_split(df_households_filtered, prop = .7)
train <- training(data_split)
test <- testing(data_split)
features <- setdiff(names(train), "price")
train_features <- train[, features]
train_class <- train$price
MLP_model <- keras_model_sequential() %>%
# The overall neural network architecture
layer_dense(units = 10, activation = "relu", input_shape = ncol(train_features)) %>%
layer_batch_normalization() %>% layer_dense(units = 5, activation = "relu") %>%
layer_batch_normalization() %>% layer_dropout(rate = 0.2) %>% layer_dense(units = 1) %>%
# Defining the optimization algorithm and loss function.
# Use binary and multi-categorical cross entropy for classification.
compile(optimizer = optimizer_rmsprop(),loss = "mse",metrics = c("mae"))
summary(MLP_model)
# train the model
learn <- MLP_model %>% fit(
x = as.matrix(train_features),
y = as.matrix(train_class),
epochs = 15,
batch_size = 32,
validation_split = .2,
verbose = TRUE
)
plot(learn)
# Defining the real test features and actual test output.
test_features <- test[, features]
test_class <- test$price
# Generating predictions for the first 5 records in the test set.
model_predictions <- MLP_model %>% predict(as.matrix(test_features[1:5,]))
model_predictions[model_predictions < 0] <- 0
model_results <- MLP_model %>% evaluate(as.matrix(test_features), as.matrix(test_class))
print(model_results)
model_results[1]
citation("keras")
citation("shapr")
citation("lightgbm")
citation("rBayesianOptimization")
library(simputation)
set.seed(1234)
df <- iris
df_NA <- as.data.frame(lapply(df, function(imp) imp[ sample(c(TRUE, NA),
prob = c(0.85, 0.15), size = length(imp), replace = TRUE)]))
median_imputed <- impute_median(df_NA, Sepal.Length)
df <- iris
df_NA <- as.data.frame(lapply(df, function(imp) imp[ sample(c(TRUE, NA),
prob = c(0.85, 0.15), size = length(imp), replace = TRUE)]))
df_NA
median_imputed <- impute_median(df_NA,
Sepal.Length ~ Species)
